# å¤–éƒ¨åŒ…è„šæœ¬ä¸å®˜æ–¹è„šæœ¬åŠŸèƒ½å¯¹æ¯”åˆ†æ

## ğŸ” æ¦‚è§ˆ

å¯¹æ¯”å‘ç°ï¼š**å¤–éƒ¨åŒ…çš„æ‰€æœ‰è„šæœ¬éƒ½è¢«ä¸¥é‡ç®€åŒ–**ï¼Œç¼ºå°‘äº†å®˜æ–¹è„šæœ¬çš„è®¸å¤šé‡è¦åŠŸèƒ½ã€‚

| è„šæœ¬ | å®˜æ–¹è¡Œæ•° | å¤–éƒ¨åŒ…è¡Œæ•° | ç®€åŒ–ç‡ | çŠ¶æ€ |
|------|---------|-----------|--------|------|
| train.py / train_rl.py | 207 | 155 | -25% | âš ï¸ ç¼ºå°‘åŠŸèƒ½ |
| play.py / play_rl.py | 194 | 136 | -30% | âš ï¸ ç¼ºå°‘åŠŸèƒ½ |
| record_demos.py | 541 | 260 | -52% | ğŸ”´ ä¸¥é‡ç®€åŒ– |
| annotate_demos.py | 460 | 460 | 0% | âœ… å®Œæ•´å¤åˆ¶ |
| generate_dataset.py | 195 | 195 | 0% | âœ… å®Œæ•´å¤åˆ¶ |
| train_bc.py (train.py) | 700+ | 700+ | 0% | âœ… å®Œæ•´å¤åˆ¶ |

---

## 1ï¸âƒ£ train_rl.py ç¼ºå°‘çš„åŠŸèƒ½

### å®˜æ–¹ train.py æœ‰ä½†å¤–éƒ¨åŒ… train_rl.py æ²¡æœ‰çš„ï¼š

| åŠŸèƒ½ | å®˜æ–¹ | å¤–éƒ¨åŒ… | è¯´æ˜ |
|------|-----|--------|------|
| **Hydra é…ç½®ç³»ç»Ÿ** | âœ… | âŒ | ä½¿ç”¨ `@hydra_task_config` è£…é¥°å™¨åŠ¨æ€åŠ è½½é…ç½® |
| **Seed è®¾ç½®** | âœ… | âŒ | `env_cfg.seed = agent_cfg.seed` |
| **å¤š GPU è®­ç»ƒ** | âœ… | âŒ | `--distributed` å‚æ•°ï¼Œæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ |
| **è§†é¢‘å½•åˆ¶** | âœ… | âŒ | `--video` å‚æ•°ï¼Œä½¿ç”¨ `gym.wrappers.RecordVideo` |
| **IO Descriptors å¯¼å‡º** | âœ… | âŒ | `export_io_descriptors` å’Œ `io_descriptors_output_dir` |
| **Git Repo è¿½è¸ª** | âœ… | âŒ | `runner.add_git_repo_to_log(__file__)` |
| **é…ç½®æ–‡ä»¶å¯¼å‡º** | âœ… | âŒ | `dump_yaml()` å’Œ `dump_pickle()` ä¿å­˜é…ç½® |
| **Multi-agent æ”¯æŒ** | âœ… | âŒ | `multi_agent_to_single_agent()` è½¬æ¢ |
| **æ›´å¤æ‚çš„ checkpoint åŠ è½½** | âœ… | âŒ | `get_checkpoint_path()` å‡½æ•° |
| **Run name è¿½åŠ ** | âœ… | âŒ | `log_dir += f"_{agent_cfg.run_name}"` |
| **CUDA ä¼˜åŒ–è®¾ç½®** | âœ… | âŒ | `torch.backends.cuda.matmul.allow_tf32 = True` |

### ä»£ç å¯¹æ¯”ç¤ºä¾‹

**å®˜æ–¹ train.pyï¼ˆç¬¬ 108-163 è¡Œï¼‰**ï¼š
```python
@hydra_task_config(args_cli.task, args_cli.agent)
def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agent_cfg: RslRlOnPolicyRunnerCfg):
    """Train with RSL-RL agent."""
    # override configurations with non-hydra CLI arguments
    agent_cfg = cli_args.update_rsl_rl_cfg(agent_cfg, args_cli)
    env_cfg.scene.num_envs = args_cli.num_envs if args_cli.num_envs is not None else env_cfg.scene.num_envs
    agent_cfg.max_iterations = (
        args_cli.max_iterations if args_cli.max_iterations is not None else agent_cfg.max_iterations
    )

    # set the environment seed
    env_cfg.seed = agent_cfg.seed
    env_cfg.sim.device = args_cli.device if args_cli.device is not None else env_cfg.sim.device

    # multi-gpu training configuration
    if args_cli.distributed:
        env_cfg.sim.device = f"cuda:{app_launcher.local_rank}"
        agent_cfg.device = f"cuda:{app_launcher.local_rank}"
        seed = agent_cfg.seed + app_launcher.local_rank
        env_cfg.seed = seed
        agent_cfg.seed = seed

    # specify directory for logging experiments
    log_root_path = os.path.join("logs", "rsl_rl", agent_cfg.experiment_name)
    log_root_path = os.path.abspath(log_root_path)

    log_dir = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    if agent_cfg.run_name:
        log_dir += f"_{agent_cfg.run_name}"  # âŒ å¤–éƒ¨åŒ…æ²¡æœ‰
    log_dir = os.path.join(log_root_path, log_dir)

    # set the IO descriptors output directory if requested
    if isinstance(env_cfg, ManagerBasedRLEnvCfg):
        env_cfg.export_io_descriptors = args_cli.export_io_descriptors  # âŒ å¤–éƒ¨åŒ…æ²¡æœ‰
        env_cfg.io_descriptors_output_dir = log_dir

    # create isaac environment
    env = gym.make(args_cli.task, cfg=env_cfg, render_mode="rgb_array" if args_cli.video else None)

    # convert to single-agent instance if required
    if isinstance(env.unwrapped, DirectMARLEnv):
        env = multi_agent_to_single_agent(env)  # âŒ å¤–éƒ¨åŒ…æ²¡æœ‰

    # save resume path before creating a new log_dir
    if agent_cfg.resume or agent_cfg.algorithm.class_name == "Distillation":
        resume_path = get_checkpoint_path(log_root_path, agent_cfg.load_run, agent_cfg.load_checkpoint)  # âŒ å¤–éƒ¨åŒ…ç®€åŒ–äº†

    # wrap for video recording
    if args_cli.video:
        video_kwargs = {...}
        env = gym.wrappers.RecordVideo(env, **video_kwargs)  # âŒ å¤–éƒ¨åŒ…æ²¡æœ‰

    # wrap around environment for rsl-rl
    env = RslRlVecEnvWrapper(env, clip_actions=agent_cfg.clip_actions)

    # create runner from rsl-rl
    runner = OnPolicyRunner(env, agent_cfg.to_dict(), log_dir=log_dir, device=agent_cfg.device)

    # write git state to logs
    runner.add_git_repo_to_log(__file__)  # âŒ å¤–éƒ¨åŒ…æ²¡æœ‰

    # load the checkpoint
    if agent_cfg.resume or agent_cfg.algorithm.class_name == "Distillation":
        runner.load(resume_path)

    # dump the configuration into log-directory
    dump_yaml(os.path.join(log_dir, "params", "env.yaml"), env_cfg)  # âŒ å¤–éƒ¨åŒ…æ²¡æœ‰
    dump_yaml(os.path.join(log_dir, "params", "agent.yaml"), agent_cfg)
    dump_pickle(os.path.join(log_dir, "params", "env.pkl"), env_cfg)
    dump_pickle(os.path.join(log_dir, "params", "agent.pkl"), agent_cfg)

    # run training
    runner.learn(num_learning_iterations=agent_cfg.max_iterations, init_at_random_ep_len=True)

    env.close()
```

**å¤–éƒ¨åŒ… train_rl.pyï¼ˆç¬¬ 79-148 è¡Œï¼‰**ï¼š
```python
def main():
    """Train the RL agent."""
    # Parse environment configuration
    env_cfg = parse_env_cfg(
        task_name=args_cli.task,
        device=args_cli.device,
        num_envs=args_cli.num_envs,
    )

    # Create environment
    env = gym.make(args_cli.task, cfg=env_cfg)  # âœ… ç®€åŒ–ï¼šæ²¡æœ‰ render_mode

    # Wrap environment for RSL-RL
    env = RslRlVecEnvWrapper(env)  # âœ… ç®€åŒ–ï¼šæ²¡æœ‰ clip_actions

    # Create runner config
    runner_cfg = DesktopOrganizerPPORunnerCfg()  # âœ… ç¡¬ç¼–ç é…ç½®ç±»
    runner_cfg.max_iterations = args_cli.max_iterations

    # Specify directory for logging experiments (root path)
    log_root_path = os.path.abspath(args_cli.log_dir)

    # Create unique log directory for this run: {timestamp}
    log_dir = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    log_dir = os.path.join(log_root_path, log_dir)

    # Save resume path before creating new log_dir
    resume_path = None
    if args_cli.resume:
        if args_cli.load_run is None:
            print("[ERROR] --load_run must be specified when using --resume")
            simulation_app.close()
            return
        resume_path = os.path.join(log_root_path, args_cli.load_run)  # âœ… ç®€åŒ–ç‰ˆ

    # Create runner
    runner = OnPolicyRunner(env, runner_cfg.to_dict(), log_dir=log_dir, device=args_cli.device)

    # Load checkpoint if resuming
    if args_cli.resume:
        runner.load(resume_path)

    # Train
    runner.learn(num_learning_iterations=args_cli.max_iterations, init_at_random_ep_len=True)

    # Save final model
    final_model_path = os.path.join(runner.log_dir, "model_final.pt")
    runner.save(final_model_path)

    # Close the simulator
    env.close()
    simulation_app.close()
```

---

## 2ï¸âƒ£ record_demos.py ç¼ºå°‘çš„åŠŸèƒ½

### å®˜æ–¹æœ‰ä½†å¤–éƒ¨åŒ…æ²¡æœ‰çš„ï¼š

| åŠŸèƒ½ | å®˜æ–¹ | å¤–éƒ¨åŒ… | è¯´æ˜ |
|------|-----|--------|------|
| **æ¨¡å—åŒ–å‡½æ•°** | âœ… 9ä¸ªè¾…åŠ©å‡½æ•° | âŒ åªæœ‰2ä¸ªå‡½æ•° | setup_output_directories, create_environment_configç­‰ |
| **UI æŒ‡ä»¤æ˜¾ç¤º** | âœ… | âŒ | `setup_ui()` å’Œ `show_subtask_instructions()` |
| **æˆåŠŸæ¡ä»¶å¤„ç†** | âœ… | âŒ | `process_success_condition()` ç‹¬ç«‹å‡½æ•° |
| **Reset å¤„ç†** | âœ… | âŒ | `handle_reset()` ç‹¬ç«‹å‡½æ•° |
| **ä»¿çœŸå¾ªç¯** | âœ… | âŒ | `run_simulation_loop()` ç‹¬ç«‹å‡½æ•° |

**ä»£ç é‡å¯¹æ¯”**ï¼š
- å®˜æ–¹ï¼š541 è¡Œï¼ˆæ¨¡å—åŒ–ï¼Œæ˜“ç»´æŠ¤ï¼‰
- å¤–éƒ¨åŒ…ï¼š260 è¡Œï¼ˆç®€åŒ–ï¼Œå¯èƒ½ç¼ºå°‘è¾¹ç•Œæƒ…å†µå¤„ç†ï¼‰

---

## 3ï¸âƒ£ å…¶ä»–è„šæœ¬çŠ¶æ€

### âœ… å®Œå…¨ä¸€è‡´çš„è„šæœ¬

- **annotate_demos.py**ï¼šå®Œæ•´å¤åˆ¶ï¼Œ460 è¡Œ
- **generate_dataset.py**ï¼šå®Œæ•´å¤åˆ¶ï¼Œ195 è¡Œ
- **train_bc.py**ï¼šå®Œæ•´å¤åˆ¶ï¼Œ700+ è¡Œ

è¿™ä¸‰ä¸ªè„šæœ¬æ˜¯ç›´æ¥ä»å®˜æ–¹å¤åˆ¶çš„ï¼Œåªæ·»åŠ äº† `import desktop_organizer`ã€‚

### âš ï¸ ç®€åŒ–çš„è„šæœ¬

- **train_rl.py**ï¼šç®€åŒ– 25%
- **play_rl.py**ï¼šç®€åŒ– 30%
- **record_demos.py**ï¼šç®€åŒ– 52%

---

## ğŸ¯ é—®é¢˜å½±å“åˆ†æ

### ä¸¥é‡æ€§åˆ†çº§

| ç¼ºå¤±åŠŸèƒ½ | å½±å“ | ä¸¥é‡æ€§ |
|---------|------|--------|
| **Hydra é…ç½®ç³»ç»Ÿ** | æ— æ³•ä½¿ç”¨å®˜æ–¹çš„é…ç½®æ–‡ä»¶ç®¡ç† | ğŸ”´ é«˜ |
| **é…ç½®æ–‡ä»¶å¯¼å‡º** | æ— æ³•è®°å½•å®Œæ•´çš„è®­ç»ƒé…ç½® | ğŸ”´ é«˜ |
| **Seed è®¾ç½®** | å®éªŒä¸å¯å¤ç° | ğŸ”´ é«˜ |
| **è§†é¢‘å½•åˆ¶** | æ— æ³•å½•åˆ¶è®­ç»ƒè§†é¢‘ | ğŸŸ¡ ä¸­ |
| **Git Repo è¿½è¸ª** | æ— æ³•è¿½è¸ªä»£ç ç‰ˆæœ¬ | ğŸŸ¡ ä¸­ |
| **å¤š GPU è®­ç»ƒ** | æ— æ³•åˆ†å¸ƒå¼è®­ç»ƒ | ğŸŸ¡ ä¸­ |
| **Multi-agent æ”¯æŒ** | åªæ”¯æŒå•æ™ºèƒ½ä½“ | ğŸŸ¢ ä½ï¼ˆæ¡Œé¢æ•´ç†æ˜¯å•æ™ºèƒ½ä½“ï¼‰|
| **IO Descriptors å¯¼å‡º** | æ— æ³•å¯¼å‡ºç¯å¢ƒæè¿° | ğŸŸ¢ ä½ |

---

## âœ… å»ºè®®æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1ï¼šå®Œå…¨å¯¹é½å®˜æ–¹è„šæœ¬ âœ¨ æ¨è

**åšæ³•**ï¼š
1. åˆ é™¤ç®€åŒ–ç‰ˆçš„ `train_rl.py` å’Œ `play_rl.py`
2. ä»å®˜æ–¹è„šæœ¬å®Œæ•´å¤åˆ¶ï¼Œåªæ·»åŠ  `import desktop_organizer`
3. ä¿ç•™å®˜æ–¹çš„æ‰€æœ‰åŠŸèƒ½å’Œå‚æ•°

**ä¼˜ç‚¹**ï¼š
- âœ… åŠŸèƒ½å®Œå…¨ä¸€è‡´
- âœ… æ”¯æŒæ‰€æœ‰é«˜çº§åŠŸèƒ½ï¼ˆseedã€videoã€multi-gpuç­‰ï¼‰
- âœ… ä¸å®˜æ–¹æ–‡æ¡£å®Œå…¨å…¼å®¹
- âœ… å®éªŒå¯å¤ç°

**ç¼ºç‚¹**ï¼š
- âŒ ä»£ç æ›´å¤æ‚ï¼ˆä½†è¿™æ˜¯å¿…è¦çš„ï¼‰
- âŒ éœ€è¦ç†è§£ Hydra é…ç½®ç³»ç»Ÿ

---

### æ–¹æ¡ˆ 2ï¼šä¿ç•™ç®€åŒ–ç‰ˆï¼ˆå½“å‰ï¼‰

**ä¼˜ç‚¹**ï¼š
- âœ… ä»£ç ç®€å•æ˜“æ‡‚
- âœ… åŸºç¡€è®­ç»ƒåŠŸèƒ½å®Œæ•´

**ç¼ºç‚¹**ï¼š
- âŒ ç¼ºå°‘å…³é”®åŠŸèƒ½ï¼ˆseedã€é…ç½®å¯¼å‡ºï¼‰
- âŒ å®éªŒä¸å¯å¤ç°
- âŒ æ— æ³•ä½¿ç”¨å®˜æ–¹é…ç½®æ–‡ä»¶
- âŒ ä¸ç”¨æˆ·è¦æ±‚ä¸ç¬¦ï¼ˆ"åŠŸèƒ½è¦å’Œisaaclabä¸­çš„å®Œå…¨ä¸€æ ·"ï¼‰

---

## ğŸ”§ éœ€è¦ä¿®å¤çš„è„šæœ¬

| ä¼˜å…ˆçº§ | è„šæœ¬ | é—®é¢˜ | å»ºè®® |
|-------|------|------|------|
| ğŸ”´ P0 | train_rl.py | ç¼ºå°‘ seedã€é…ç½®å¯¼å‡ºç­‰å…³é”®åŠŸèƒ½ | **å®Œæ•´å¤åˆ¶å®˜æ–¹ train.py** |
| ğŸ”´ P0 | record_demos.py | ç®€åŒ–è¿‡åº¦ï¼Œå¯èƒ½æœ‰ bug | **å®Œæ•´å¤åˆ¶å®˜æ–¹ record_demos.py** |
| ğŸŸ¡ P1 | play_rl.py | ç¼ºå°‘éƒ¨åˆ†åŠŸèƒ½ | **å®Œæ•´å¤åˆ¶å®˜æ–¹ play.py** |
| âœ… P2 | annotate_demos.py | å·²å®Œæ•´ | ä¿æŒä¸å˜ |
| âœ… P2 | generate_dataset.py | å·²å®Œæ•´ | ä¿æŒä¸å˜ |
| âœ… P2 | train_bc.py | å·²å®Œæ•´ | ä¿æŒä¸å˜ |

---

## ğŸ“‹ è¡ŒåŠ¨è®¡åˆ’

### ç«‹å³ä¿®å¤ï¼ˆP0ï¼‰

1. **åˆ é™¤** `/root/isaaclab-desktop-organizer/scripts/train_rl.py`
2. **å¤åˆ¶** å®˜æ–¹ `scripts/reinforcement_learning/rsl_rl/train.py` â†’ `/root/isaaclab-desktop-organizer/scripts/train_rl.py`
3. **æ·»åŠ ** `import desktop_organizer  # noqa: F401` åˆ°å¯¼å…¥éƒ¨åˆ†
4. **æµ‹è¯•** ç¡®ä¿æ‰€æœ‰å‚æ•°å’ŒåŠŸèƒ½æ­£å¸¸

5. **åˆ é™¤** `/root/isaaclab-desktop-organizer/scripts/record_demos.py`
6. **å¤åˆ¶** å®˜æ–¹ `scripts/tools/record_demos.py` â†’ `/root/isaaclab-desktop-organizer/scripts/record_demos.py`
7. **æ·»åŠ ** `import desktop_organizer  # noqa: F401`
8. **æµ‹è¯•** å½•åˆ¶åŠŸèƒ½

### åç»­ä¿®å¤ï¼ˆP1ï¼‰

9. åŒæ ·å¤„ç† `play_rl.py`

---

## ğŸ’¡ å…³é”®å­¦ä¹ 

ç”¨æˆ·çš„è¦æ±‚æ˜¯å¯¹çš„ï¼š**"åŠŸèƒ½è¦å’Œisaaclabä¸­çš„å®Œå…¨ä¸€æ ·ï¼Œä¸èƒ½è‡ªå·±åŠ "**

æˆ‘ä»¬çŠ¯çš„é”™è¯¯æ˜¯**è¿‡åº¦ç®€åŒ–**ï¼Œè€Œä¸æ˜¯"è‡ªå·±æ·»åŠ åŠŸèƒ½"ã€‚æ­£ç¡®çš„åšæ³•åº”è¯¥æ˜¯ï¼š
- âœ… å®Œæ•´å¤åˆ¶å®˜æ–¹è„šæœ¬
- âœ… åªæ·»åŠ  `import desktop_organizer`
- âŒ ä¸è¦ç®€åŒ–æˆ–ä¿®æ”¹ä»»ä½•é€»è¾‘

---

**åˆ›å»ºæ—¥æœŸ**ï¼š2026-01-27
**çŠ¶æ€**ï¼šğŸ”´ å‘ç°ä¸¥é‡é—®é¢˜ï¼Œéœ€è¦ç«‹å³ä¿®å¤
